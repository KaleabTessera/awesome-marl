### Convergent Learning

<details> <summary> <a href="http://www.dklevine.com/archive/refs4415.pdf"> Learning Mixed Equilibria </a> by Drew Fundenberg, David M. Kreps. Journal of Games and Economic Behvaior, 1993.  </summary> We study learning processes for finite strategic-form games, in which players use the history of past play to forecast play in the current period. In a generalization of fictitious play, we assume only that players asymptotically choose best responses to the historical frequencies of opponents′ past play. This implies that if the stage-game strategies converge, the limit is a Nash equilibrium. In the basic model, plays seems unlikely to converge to a mixed-strategy equilibrium, but such convergence is natural when the stage game is perturbed in the manner of Harsanyi′s purification theorem.  <br> - </details>

<details> <summary> <a href="https://www.aaai.org/Papers/AAAI/1998/AAAI98-106.pdf"> The Dynamics of Reinforcement Learning in Cooperative Multiagent Systems </a> by Caroline Claus, Craig Boutilier. AAAI, 1998.  </summary> Reinforcement learning can provide a robust and natural means for agents to learn how to coordinate their action choices in multiagent systems. We examine some of the factors that can influence the dynamics of the learning process in such a setting. We first distinguish reinforcement learners that are unaware of (or ignore) the presence of other agents from those that explicitly attempt to learn the value of joint actions and the strategies of their counterparts. We study (a simple form of) Q-learning in cooperative multiagent systems under these two perspectives, focusing on the influence of that game structure and exploration strategies on convergence to (optimal and suboptimal) Nash equilibria. We then propose alternative optimistic exploration strategies that increase the likelihood of convergence to an optimal equilibrium.  <br> - </details>

<details> <summary> <a href="https://econweb.ucsd.edu/~jandreon/Econ264/papers/Erev%20Roth%20AER%201998.pdf"> Predicting how people play games: reinforcement leaning in experimental games with unique,mixed strategy equilibria </a> by Ido Erev, Alvin E. Roth. The American Economic Review, 1998. </summary> We examine learning in all experiments we could locate involving 100 periods or more of games with a unique equilibrium in mixed strategies, and in a new experiment. We study both the ex post ("best fit") descriptive power of learning models, and their ex ante predictive power, by simulating each experiment using parameters estimated from the other experiments. Even a one-parameter reinforcement learning model robustly outperforms the equilibrium predictions. Predictive power is improved by adding "forgetting" and "experimentation," or by allowing greater rationality as in probabilistic fictitious play. Implications for developing a low-rationality, cognitive game theory are discussed. <br> - </details>

<details> <summary> <a href="https://webdocs.cs.ualberta.ca/~bowling/papers/00icml.pdf"> Convergence Problems of General-Sum Multiagent Reinforcement Learning </a>by Michael Bowling. ICML, 2000. </summary> Stochastic games are a generalization of MDPs to multiple agents, and can be used as a framework for investigating multiagent learning. Hu and Wellman (1998) recently proposed a multiagent Q-learning method for general-sum stochastic games. In addition to describing the algorithm, they provide a proof that the method will converge to a Nash equilibrium for the game under specified conditions. The convergence depends on a lemma stating that the iteration used by this method is a contraction mapping. Unfortunately the proof is incomplete. In this paper we present a counterexample and flaw to the lemma’s proof. We also introduce strengthened assumptions under which the lemma holds, and examine how this affects the classes of games to which the theoretical result can be applied  <br> - </details>

<details> <summary> <a href="https://www.cs.cmu.edu/~mmv/papers/01ijcai-mike.pdf"> Rational and Convergent Learning in Stochastic Games </a>by Michael Bowling, Manuela Veloso. 2001. </summary> This paper investigates the problem of policy learning in multiagent environments using the stochastic game framework, which we briefly overview. We introduce two properties as desirable for a learning agent when in the presence of other learning agents, namely rationality and convergence. We examine existing reinforcement learning algorithms according to these two properties and notice that they fail to simultaneously meet both criteria. We then contribute a new learning algorithm, WoLF policy hillclimbing, that is based on a simple principle: “learn quickly while losing, slowly while winning.” The algorithm is proven to be rational and we present empirical results for a number of stochastic games showing the algorithm converges.  <br> - </details>

<details> <summary> <a href="https://dspace.mit.edu/bitstream/handle/1721.1/3688/CS023.pdf?sequence=2&isAllowed=y"> Playing is believing: the role of beliefs in multi-agent learning </a> by Yu-Han Chang, Leslie Pack Kaelbling. NeurIPS, 2001. </summary> We propose a new classification for multi-agent learning algorithms, with each league of players characterized by both their possible strategies and possible beliefs. Using this classification, we review the optimality of existing algorithms and discuss some insights that can be gained. We propose an incremental improvement to the existing algorithms that seems to achieve average payoffs that are at least the Nash equilibrium payoffs in the long-run against fair opponents. <br> - </details>
