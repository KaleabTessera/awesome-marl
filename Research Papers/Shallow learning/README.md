This is a collection of older papers on the topic of learning in multi-agent systems. We sort papers by publication date and survey subtopic. Any additions to this repo are welcome.

### Convergent Learning

<details> <summary> <a href="http://www.dklevine.com/archive/refs4415.pdf"> Learning Mixed Equilibria </a> by Drew Fundenberg, David M. Kreps. Journal of Games and Economic Behvaior, 1993.  </summary> We study learning processes for finite strategic-form games, in which players use the history of past play to forecast play in the current period. In a generalization of fictitious play, we assume only that players asymptotically choose best responses to the historical frequencies of opponents′ past play. This implies that if the stage-game strategies converge, the limit is a Nash equilibrium. In the basic model, plays seems unlikely to converge to a mixed-strategy equilibrium, but such convergence is natural when the stage game is perturbed in the manner of Harsanyi′s purification theorem.  <br> - </details>

<details> <summary> <a href="http://www.eecs.harvard.edu/cs286r/courses/spring06/papers/kalailehrer93.pdf"> Rational learning leads to nash equilibrium </a>by Ehud Kalai, Ehud Lehrer. Econometrica, 1993. <a href="link">  </a> </summary> Each of n players, in an infinitely repeated game, starts with subjective beliefs about his opponents' strategies. If the individual beliefs are compatible with the true strategies chosen, then Bayesian updating will lead in the long run to accurate prediction of the future play of the game. It follows that individual players, who know their own payoff matrices and choose strategies to maximize their expected utility, must eventually play according to a Nash equilibrium of the repeated game. An immediate corollary is that, when playing a Harsanyi-Nash equilibrium of a repeated game of incomplete information about opponents' payoff matrices, players will eventually play a Nash equilibrium of the real game, as if they had complete information <br> - </details>

<details> <summary> <a href="https://cs.brown.edu/~mlittman/papers/ml96-generalized.pdf"> A generalized reinforcement-learning model: Convergence and applications </a>by Michael L. Littman, C. Szepesvari. ICML, 1996. <a href="link">  </a> </summary> Reinforcement learning is the process by which an autonomous agent uses its experience interacting with an environment to improve its behavior. The Markov decision process (MDP) model is a popular way of formalizing the reinforcement learning problem but it is by no means the only way. In this paper we show how many of the important theoretical results concerning reinforcement learning in MDPs extend to a generalized MDP model that includes MDPs two-player games and MDPs under a worst-case optimality criterion as special cases. The basis of this extension is a stochastic approximation theorem that reduces asynchronous convergence to synchronous con <br> - </details>

<details> <summary> <a href="https://www.aaai.org/Papers/AAAI/1998/AAAI98-106.pdf"> The Dynamics of Reinforcement Learning in Cooperative Multiagent Systems </a> by Caroline Claus, Craig Boutilier. AAAI, 1998.  </summary> Reinforcement learning can provide a robust and natural means for agents to learn how to coordinate their action choices in multiagent systems. We examine some of the factors that can influence the dynamics of the learning process in such a setting. We first distinguish reinforcement learners that are unaware of (or ignore) the presence of other agents from those that explicitly attempt to learn the value of joint actions and the strategies of their counterparts. We study (a simple form of) Q-learning in cooperative multiagent systems under these two perspectives, focusing on the influence of that game structure and exploration strategies on convergence to (optimal and suboptimal) Nash equilibria. We then propose alternative optimistic exploration strategies that increase the likelihood of convergence to an optimal equilibrium.  <br> - </details>

<details> <summary> <a href="https://econweb.ucsd.edu/~jandreon/Econ264/papers/Erev%20Roth%20AER%201998.pdf"> Predicting how people play games: reinforcement leaning in experimental games with unique, mixed strategy equilibria </a> by Ido Erev, Alvin E. Roth. The American Economic Review, 1998. </summary> We examine learning in all experiments we could locate involving 100 periods or more of games with a unique equilibrium in mixed strategies, and in a new experiment. We study both the ex post ("best fit") descriptive power of learning models, and their ex ante predictive power, by simulating each experiment using parameters estimated from the other experiments. Even a one-parameter reinforcement learning model robustly outperforms the equilibrium predictions. Predictive power is improved by adding "forgetting" and "experimentation," or by allowing greater rationality as in probabilistic fictitious play. Implications for developing a low-rationality, cognitive game theory are discussed. <br> - </details>

<details> <summary> <a href="https://www.cs.cmu.edu/~mmv/papers/01ijcai-mike.pdf"> Rational and Convergent Learning in Stochastic Games </a>by Michael Bowling, Manuela Veloso. 2001. </summary> This paper investigates the problem of policy learning in multiagent environments using the stochastic game framework, which we briefly overview. We introduce two properties as desirable for a learning agent when in the presence of other learning agents, namely rationality and convergence. We examine existing reinforcement learning algorithms according to these two properties and notice that they fail to simultaneously meet both criteria. We then contribute a new learning algorithm, WoLF policy hillclimbing, that is based on a simple principle: “learn quickly while losing, slowly while winning.” The algorithm is proven to be rational and we present empirical results for a number of stochastic games showing the algorithm converges.  <br> - </details>

<details> <summary> <a href="https://dspace.mit.edu/bitstream/handle/1721.1/3688/CS023.pdf?sequence=2&isAllowed=y"> Playing is believing: the role of beliefs in multi-agent learning </a> by Yu-Han Chang, Leslie Pack Kaelbling. NeurIPS, 2001. </summary> We propose a new classification for multi-agent learning algorithms, with each league of players characterized by both their possible strategies and possible beliefs. Using this classification, we review the optimality of existing algorithms and discuss some insights that can be gained. We propose an incremental improvement to the existing algorithms that seems to achieve average payoffs that are at least the Nash equilibrium payoffs in the long-run against fair opponents. <br> - </details>

<details> <summary> <a href="https://papers.nips.cc/paper/2002/file/0d73a25092e5c1c9769a9f3255caa65a-Paper.pdf"> Efficient learning equilibrium </a>by Ronen I. Brafman, Moshe Tennenholtz. NeurIPS, 2002. <a href="link">  </a> </summary> We introduce efficient learning equilibrium (ELE), a normative approach to learning in noncooperative settings. In ELE, the learning algorithms themselves are required to be in equilibrium. In addition, the learning algorithms must arrive at a desired value after polynomial time, and a deviation from the prescribed ELE becomes irrational after polynomial time. We prove the existence of an ELE (where the desired value is the expected payoff in a Nash equilibrium) and of a Pareto-ELE (where the objective is the maximization of social surplus) in repeated games with perfect monitoring. We also show that an ELE does not always exist in the imperfect monitoring case. Finally, we discuss the extension of these results to general-sum stochastic games <br> - </details>

<br/>

### General-sum games

<details> <summary> <a href="https://webdocs.cs.ualberta.ca/~bowling/papers/00icml.pdf"> Convergence Problems of General-Sum Multiagent Reinforcement Learning </a>by Michael Bowling. ICML, 2000. </summary> Stochastic games are a generalization of MDPs to multiple agents, and can be used as a framework for investigating multiagent learning. Hu and Wellman (1998) recently proposed a multiagent Q-learning method for general-sum stochastic games. In addition to describing the algorithm, they provide a proof that the method will converge to a Nash equilibrium for the game under specified conditions. The convergence depends on a lemma stating that the iteration used by this method is a contraction mapping. Unfortunately the proof is incomplete. In this paper we present a counterexample and flaw to the lemma’s proof. We also introduce strengthened assumptions under which the lemma holds, and examine how this affects the classes of games to which the theoretical result can be applied  <br> - </details>

<details> <summary> <a href="https://jmvidal.cse.sc.edu/library/littman01a.pdf"> Friend-or-foe Q-learning in general-sum games </a>by Michael L. Littman. ICML, 2001. <a href="link">  </a> </summary> This paper describes an approach to reinforcement learning in multiagent general-sum games in which a learner is told to treat each other agent as either a friend" or foe". This Q-learning-style algorithm provides strong convergence guarantees compared to an existing Nash-equilibrium-based learning rule. <br> - </details>

<details> <summary> <a href="https://www.aaai.org/Papers/Symposia/Spring/2002/SS-02-02/SS02-02-012.pdf"> Correlated-Q Learning </a>by Amy Greenwald, Keith Hall, Roberto Serrano. NeurIPS workshop on multi-agent learning, 2002. <a href="link">  </a> </summary> Bowling named two desiderata for multiagent learning algorithms: rationality and convergence. This paper introduces correlated-Q learning, a natural generalization of Nash-Q and FF-Q that satisfies these criteria. Nash-Q satisfies rationality, but in general it does not converge. FF-Q satisfies convergence, but in general it is not rational. Correlated-Q satisfies rationality by construction. This papers demonstrates the empirical convergence of correlated-Q on a standard testbed of general-sum Markov games. <br> - </details>

<details> <summary> <a href="https://www.jmlr.org/papers/volume4/hu03a/hu03a.pdf"> Nash Q-Learning for General-Sum Stochastic Games </a>by Junling Hu, Michael Wellman. JMLR, 2003. <a href="link">  </a> </summary> We extend Q-learning to a noncooperative multiagent context, using the framework of general-sum stochastic games. A learning agent maintains Q-functions over joint actions, and performs updates based on assuming Nash equilibrium behavior over the current Q-values. This learning protocol provably converges given certain restrictions on the stage games (defined by Q-values) that arise during learning. Experiments with a pair of two-player grid games suggest that such restrictions on the game structure are not necessarily required. Stage games encountered during learning in both grid environments violate the conditions. However, learning consistently converges in the first grid game, which has a unique equilibrium Q-function, but sometimes fails to converge in the second, which has three different equilibrium Q-functions. In a comparison of offline learning performance in both games, we find agents are more likely to reach a joint optimal path with Nash Q-learning than with a single-agent Q-learning method. When at least one agent adopts Nash Q-learning, the performance of both agents is better than using single-agent Q-learning. We have also implemented an online version of Nash Q-learning that balances exploration with exploitation, yielding improved performance. <br> - </details>

<br/>

### Repeated games

<details> <summary> <a href="http://www-stat.wharton.upenn.edu/~steele/Resources/Projects/SequenceProject/Hannan.pdf"> Approximation to bayes risk in repeated plays </a>by James Hannan. Contributions to the Theory of Games, 1959. <a href="link">  </a> </summary> This paper is concerned with the development of a dynamic theoryof decision under uncertainty. The results obtained are directly applicableto the development of a dynamic theory of games in which at least one play­er is, at each stage, fully informed on the joint empirical distribution ofthe past choices of strategies of the rest. Since the decision problem canbe Imbedded in a sufficiently unspecified game theoretic model, the paperis written in the language and notation of the general two person game, in which, however, player  I’s motivation is completely unspecified. <br> - </details>

<details> <summary> <a href="https://scholars.huji.ac.il/sites/default/files/abrahamn/files/bounded.pdf"> Bounded complexity justifies cooperation in finitely repeated prisoner’s dilemma </a>by Abraham Neyman. Economic Letters, 1985. <a href="link">  </a> </summary> Cooperation in the finitely repeated prisoner's dilemma is justified, without departure from strict utility maximization or complete information, but under the assumption that there are bounds (possibly very large) to the complexity of the strategies that the players may use. <br> - </details>

<details> <summary> <a href="http://www.econ.ucla.edu/workingpapers/wp735.pdf"> Noncomputable strategies and discounted repeated games </a>by John H. Nachbar, William R. Zame. Economic Theory, 1996. <a href="link">  </a> </summary> A number of authors have used formal models of computation to capture the idea of “bounded rationality” in repeated games. Most of this literature has used computability by a finite automaton as the standard. A conceptual difficulty with this standard is that the decision problem is not “closed.” That is, for every strategy implementable by an automaton, there is some best response implementable by an automaton, but there may not exist any algorithm forfinding such a best response that can be implemented by an automaton. However, such algorithms can always be implemented by a Turing machine, the most powerful formal model of computation. In this paper, we investigate whether the decision problem can be closed by adopting Turing machines as the standard of computability. The answer we offer is negative. Indeed, for a large class of discounted repeated games (including the repeated Prisoner's Dilemma) there exist strategies implementable by a Turing machine for whichno best response is implementable by a Turing machine. <br> - </details>

<details> <summary> <a href="https://www.cs.utexas.edu/users/pstone/Papers/bib2html-links/threats-ATAL2001.pdf"> Implicit negotiation in repeated games </a>by Peter Stone and Michael L. Littman. ATAL, 2001. <a href="link">  </a> </summary> In business-related interactions such as the on-going high-stakes FCC spectrum auctions, explicit communication among participants is regarded as collusion, and is therefore illegal. In this paper, we consider the possibility of autonomous agents engaging in implicit negotiation via their tacit interactions. In repeated general-sum games, our testbed for studying this type of interaction, an agent using a ``best response'' strategy maximizes its own payoff assuming its behavior has no effect on its opponent. This notion of best response requires some degree of learning to determine the fixed opponent behavior. Against an unchanging opponent, the best-response agent performs optimally, and can be thought of as a ``follower, '' since it adapts to its opponent. However, pairing two best-response agents in a repeated game can result in suboptimal behavior. We demonstrate this suboptimality in several different games using variants of Q-learning as an example of a best-response strategy. We then examine two ``leader'' strategies that induce better performance from opponent followers via stubbornness and threats. These tactics are forms of implicit negotiation in that they aim to achieve a mutually beneficial outcome without using explicit communication outside of the game. <br> - </details>

<details> <summary> <a href="https://www.cs.cmu.edu/~mmv/papers/01ijcai-mike.pdf"> Sophisticated EWA Learning and Strategic Teaching in Repeated Games </a>by Colin F. Camerer, Teck-Hua Ho, Juin-Kuan Chong. Journal of Economic Theory, 2002. <a href="https://www.Summary.so/instadeep/Multiagent-Learning-Basics-Challenges-and-Prospect-21cb7b4294b84a4188cafd184a3deed8">   </a> </summary> Most learning models assume players are adaptive (i.e., they respond only to their own previous experience and ignore others' payo® information) and behavior is not sensitive to the way in which players are matched. Empirical evidence suggests otherwise. In this paper, we extend our adaptive experienceweighted attraction (EWA) learning model to capture sophisticated learning and strategic teaching in repeated games. The generalized model assumes there is a mixture of adaptive learners and sophisticated players. An adaptive learner adjusts his behavior the EWA way. A sophisticated player rationally best-responds to her forecasts of all other behaviors. A sophisticated player can be either myopic or farsighted. A farsighted player develops multiple-period rather than single-period forecasts of others' behaviors and chooses to `teach' the other players by choosing a strategy scenario that gives her the highest discounted net present value. We estimate the model using data from p-beauty contests and repeated trust games with incomplete information. The generalized model is better than the adaptive EWA model in describing and predicting behavior. Including teaching also allows an empirical learning-based approach to reputation formation which predicts better than a quantal-response extension of the standard typebased approach. <br> - </details>

<br/>

### Opponent modelling

<details> <summary> <a href="http://strategicreasoning.org/wp-content/uploads/2010/03/csr01.pdf"> Learning about other agents in a dynamic multiagent system </a>by Junling Hu, Michael Wellman. Journal of Cognitive Systems Research, 2001. <a href="link">  </a> </summary> We analyze the problem of learning about other agents in a class of dynamic multiagent systems, where performance of the primary agent depends on behavior of the others. We consider an online version of the problem, where agents must learn models of the others in the course of continual interactions. Various levels of recursive models are implemented in a simulated double auction market. Our experiments show learning agents on average outperform non-learning agents who do not use information about others. Among learning agents, those with minimum recursion assumption generally perform better than the agents with more complicated, though often wrong assumptions. <br> - </details>

<br/>

### Decision theory

<details> <summary> <a href="https://www.ijcai.org/Proceedings/91-1/Papers/011.pdf"> A decision-theoretic approach to coordinating multiagent interactions </a>by Piotr J. Gmytrasiewicz, Edmund H. Durfee, David K. Weh. IJCAI, 1991. <a href="https://www.Summary.so/instadeep/Multiagent-Learning-Basics-Challenges-and-Prospect-21cb7b4294b84a4188cafd184a3deed8">  </a> </summary> We describe a decision-theoretic method that an autonomous agent can use to model multiagent situations and behave rationally based on its model. Our approach, which we call the Recursive Modeling Method, explicitly accounts for the recursive nature of multiagent reasoning. Our method lets an agent recursively model another agent's decisions based on probabilistic views of how that agent perceives the multiagent situation, which in turn are derived from hypothesizing how that other agent perceives the initial agent's possible decisions, and so on. Further, we show how the possibility of multiple interactions can affect the decisions of agents, allowing cooperative behavior to emerge as a rational choice of selfish agents that otherwise might behave uncooperatively <br> - </details>

<br/>

### Extensive form games

<details> <summary> <a href="https://www.tau.ac.il/~samet/papers/learning-to-play.pdf">  Learning to play games in extensive form by valuation </a>by Phillipe Jehiel, Dov Samet. NAJ Economics, 2001. <a href="link">  </a> </summary> Game theoretic models of learning which are based on the strategic form of the game cannot explain learning in games with large extensive form. We study learning in such games by using valuation of moves. A valuation for a player is a numeric assessment of her moves that purports to reflect their desirability. We consider a myopic player, who chooses moves with the highest valuation. Each time the game is played, the player revises her valuation by assigning the payoff obtained in the play to each of the moves she has made. We show for a repeated win–lose game that if the player has a winning strategy in the stage game, there is almost surely a time after which she always wins. When a player has more than two payoffs, a more elaborate learning procedure is required. We consider one that associates with each move the average payoff in the rounds in which this move was made. When all players adopt this learning procedure, with some perturbations, then, with probability 1 there is a time after which strategies that are close to subgame perfect equilibrium are played. A single player who adopts this procedure can guarantee only her individually rational payoff <br> - </details>

<br/>

### Theoretical frameworks for MARL

<details> <summary> <a href="https://courses.cs.duke.edu/spring07/cps296.3/littman94markov.pdf"> Markov games as a framework for multiagent reinforcement learning  </a>by Michael L. Littman. ICML, 1994. <a href="link">  </a> </summary> In the Markov decision process(MDP) formalization of reinforcement learning, a single adaptive agent interacts with an environment defined by a probabilistic transition function. In this solipsistic view, secondary agents can only be part of the environment and are therefore fixed in their behavior. The framework of Markov games allows us to widen this view to include multiple adaptive agents with interacting or competing goals. This paper considers a step in this direction in which exactly two agents with diametrically opposed goals share an environment. It describes a Q-learning-like algorithm for finding optimal policies and demonstrates its application to a simple two-player game in which the optimal policy is probabilistic. <br> - </details>

<details> <summary> <a href="https://www.lirmm.fr/~jq/Cours/3cycle/module/HuWellman98icml.pdf"> Multiagent Reinforcement Learning: Theoretical Framework and an Algorithm </a>by Junling Hu, Michael P. Wellman. ICML, 1998. <a href="link">  </a> </summary> In this paper, we adopt general-sum stochastic games as a framework for multiagent reinforcement learning. Our work extends previous work by Littman on zero-sum stochastic games to a broader framework. We design a multiagent Q-learning method under this framework, and prove that it converges to a Nash equilibrium under specified conditions. This algorithm is useful for finding the optimal strategy when there exists a unique Nash equilibrium in the game. When there exist multiple Nash equilibria in the game, this algorithm should be combined with other learning techniques to find optimal strategies. <br> - </details>

<br/>

### Incomplete information games

<details> <summary> <a href="http://www.ma.huji.ac.il/~zamir/papers/22_IJGT85.pdf"> Formulation of bayesian analysis for games with incomplete information </a>by J-F. Mertens, S. Zamir. International Journal of Game Theory, 1985. <a href="link">  </a> </summary> A formal model is given of Harsanyi's infinite hierarchies of beliefs. It is shown that the model closes with some Bayesian game with incomplete information, and that any such game can be approximated by one with a finite number of states of world. <br> - </details>

<br/>

### Bounded rationality

<details> <summary> <a href="https://dl.acm.org/doi/pdf/10.1145/195058.195445"> On complexity as bounded rationality  </a>by C.H. Papadimitriou, M. Yannakakis. STOC, 1994. <a href="link">  </a> </summary> It has been hoped that computational approaches can help resolve some well-known paradoxes in game theory. We prove that tf the repeated prisoner’s dilemma M played by finite automata with less than exponentially (in the number of rounds) many states, then cooperation can be achieved an equilibrium (while with exponentially many states, defection is the only equilibrium). We furthermore prove a generalization to arbitrary games and Pareto optimal points. Finally, we present a general model of polynomially computable games, and characterize in terms of fami!iar complexity classes ranging from NP to NEXP the natural problems that arise in relation with such games. <br> - </details>

<details> <summary> <a href="https://mitpress.mit.edu/9780262681001/modeling-bounded-rationality/"> Modeling Bounded Rationality </a>by Ariel Rubinstein. MIT Press,
1998. <a href="link">  </a> </summary> The notion of bounded rationality was initiated in the 1950s by Herbert Simon; only recently has it influenced mainstream economics. In this book, Ariel Rubinstein defines models of bounded rationality as those in which elements of the process of choice are explicitly embedded. The book focuses on the challenges of modeling bounded rationality, rather than on substantial economic implications. In the first part of the book, the author considers the modeling of choice. After discussing some psychological findings, he proceeds to the modeling of procedural rationality, knowledge, memory, the choice of what to know, and group decisions.In the second part, he discusses the fundamental difficulties of modeling bounded rationality in games. He begins with the modeling of a game with procedural rational players and then surveys repeated games with complexity considerations. He ends with a discussion of computability constraints in games. The final chapter includes a critique by Herbert Simon of the author's methodology and the author's response. <br> - </details>

<br/>

### Coordination

<details> <summary> <a href="https://www.aaai.org/Papers/AAAI/1994/AAAI94-065.pdf"> Learning to coordinate without sharing information </a>by Sandip Sen, Mahendra Sekaran, John Hale. National Conference on Artificial Intelligence, 1994. <a href="link">  </a> </summary> Researchers in the field of Distributed Artificial Intelligence (DAI) have been developing efficient mechanisms to coordinate the activities of multiple autonomous agents. The need for coordination arises because agents have to share resources and expertise required to achieve their goals. Previous work in the area includes using sophisticated information exchange protocols, investigating heuristics for negotiation, and developing formal models of possibilities of conflict and cooperation among agent interests. In order to handle the changing requirements of continuous and dynamic environments, we propose learning as a means to provide additional possibilities for effective coordination. We use reinforcement learning techniques on a block pushing problem to show that agents can learn complimentary policies to follow a desired path without any knowledge about each other. We theoretically analyze and experimentally verify the effects of learning rate on system convergence, and demonstrate benefits of using learned coordination knowledge on similar problems. Reinforcement learning based coordination can be achieved in both cooperative and non-cooperative domains, and in domains with noisy communication channels and other stochastic characteristics that present a formidable challenge to using other coordination schemes. <br> - </details>

<br/>

<!-- <details> <summary> <a href="link"> title </a>by authors. Conference, year. <a href="link">  </a> </summary> abstract <br> - </details> -->
